{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setting up the libraries needed\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T16:08:56.116672Z","iopub.execute_input":"2025-10-31T16:08:56.117016Z","iopub.status.idle":"2025-10-31T16:08:56.123278Z","shell.execute_reply.started":"2025-10-31T16:08:56.116993Z","shell.execute_reply":"2025-10-31T16:08:56.122186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading and Exploring the Data\n\nFirst, we are going to read the data and then we are going to try to understand its variables and statistics belong it.","metadata":{}},{"cell_type":"code","source":"# Reading the data\n\ndf = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\n\npd.set_option(\"display.max_columns\", 999)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:57:16.989651Z","iopub.execute_input":"2025-11-01T08:57:16.990287Z","iopub.status.idle":"2025-11-01T08:57:17.016169Z","shell.execute_reply.started":"2025-11-01T08:57:16.990255Z","shell.execute_reply":"2025-11-01T08:57:17.015144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Our data has {df.shape[0]} rows and {df.shape[1]} columns\\nAnd the first 5 rows is looking like this:\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:57:17.487146Z","iopub.execute_input":"2025-11-01T08:57:17.487485Z","iopub.status.idle":"2025-11-01T08:57:17.531483Z","shell.execute_reply.started":"2025-11-01T08:57:17.487462Z","shell.execute_reply":"2025-11-01T08:57:17.530388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False).head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:57:18.305007Z","iopub.execute_input":"2025-11-01T08:57:18.305368Z","iopub.status.idle":"2025-11-01T08:57:18.318492Z","shell.execute_reply.started":"2025-11-01T08:57:18.305344Z","shell.execute_reply":"2025-11-01T08:57:18.317513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see, there are many missing values in our dataset.\n\nThe important things to notice here are:\n\n- Variables such as *PoolQC*, *MiscFeature*, *Alley*, etc., are **nominal variables**.  \n  The missing values in these columns indicate the *absence* of that particular feature in the house.\n\n- Another important point is that for the *Garage*-related variables, all of them are missing at the same count.  \n  This means that these houses **do not have a garage**, which explains why those values are missing.","metadata":{}},{"cell_type":"markdown","source":"## Identifying Variable Types","metadata":{}},{"cell_type":"code","source":"class HouseRules(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n\n    This class handles special domain-specific imputation and feature cleaning rules,\n    including:\n      - Filling categorical features that indicate absence (e.g., PoolQC, Fence, Alley)\n        with \"None\".\n      - Setting numerical features (e.g., Basement or Garage areas) to 0 when the\n        corresponding categorical indicators show that the feature is absent.\n      - Filling missing 'LotFrontage' values using the median of each Neighborhood\n        (learned during fit). If the Neighborhood is unseen, the global median is used.\n      - Converting 'MSSubClass' to a string so it is treated as a nominal categorical\n        variable instead of numeric.\n\n    The learned medians are stored during `fit()` and applied consistently during\n    `transform()`, ensuring no data leakage between training and validation/test sets.\n    \"\"\"\n\n    \n    def __init__(self):\n        self.nb_lf_med_ = None\n        self.lf_med_ = None\n        self.none_cats = [\n            \"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\n            \"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n            \"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\n            \"MasVnrType\"\n        ]\n        self.bsmt_nums = [\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\n                          \"TotalBsmtSF\",\"BsmtFullBath\",\"BsmtHalfBath\"]\n\n    def fit(self, X, y=None):\n        X = X.copy()\n        if {\"Neighborhood\",\"LotFrontage\"}.issubset(X.columns):\n            self.nb_lf_med_ = X.groupby(\"Neighborhood\")[\"LotFrontage\"].median()\n            self.lf_med_ = X[\"LotFrontage\"].median()\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # MSSubClass: kod -> nominal metin\n        if \"MSSubClass\" in X.columns:\n            X[\"MSSubClass\"] = X[\"MSSubClass\"].astype(str)\n\n        # Var/yok kategorikler → \"None\"\n        for c in self.none_cats:\n            if c in X.columns:\n                X[c] = X[c].fillna(\"None\")\n\n        # Garaj yoksa sayılsallar 0\n        if \"GarageQual\" in X.columns:\n            for c in [\"GarageYrBlt\",\"GarageCars\",\"GarageArea\"]:\n                if c in X.columns:\n                    X.loc[X[\"GarageQual\"]==\"None\", c] = 0\n\n        # MasVnrType None ise alan 0\n        if {\"MasVnrType\",\"MasVnrArea\"}.issubset(X.columns):\n            X.loc[X[\"MasVnrType\"]==\"None\", \"MasVnrArea\"] = 0\n\n        # Bodrum yoksa bodrum sayılsallar 0\n        need = {\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\"}\n        if need.issubset(X.columns):\n            mask = (X[\"BsmtQual\"]==\"None\") & (X[\"BsmtCond\"]==\"None\") & \\\n                   (X[\"BsmtExposure\"]==\"None\") & (X[\"BsmtFinType1\"]==\"None\") & \\\n                   (X[\"BsmtFinType2\"]==\"None\")\n            for c in self.bsmt_nums:\n                if c in X.columns:\n                    X.loc[mask, c] = 0\n\n        # LotFrontage mahalle medyanı\n        if self.nb_lf_med_ is not None and {\"Neighborhood\",\"LotFrontage\"}.issubset(X.columns):\n            X[\"LotFrontage\"] = X[\"LotFrontage\"].fillna(X[\"Neighborhood\"].map(self.nb_lf_med_))\n            X[\"LotFrontage\"] = X[\"LotFrontage\"].fillna(self.lf_med_)\n\n        return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:57:22.354770Z","iopub.execute_input":"2025-11-01T08:57:22.355134Z","iopub.status.idle":"2025-11-01T08:57:22.369355Z","shell.execute_reply.started":"2025-11-01T08:57:22.355110Z","shell.execute_reply":"2025-11-01T08:57:22.368134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ORDINAL_ORDER = {\n    \"ExterQual\": [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"ExterCond\": [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"HeatingQC\": [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"KitchenQual\": [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"FireplaceQu\": [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"GarageQual\": [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"GarageCond\": [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"PoolQC\": [\"None\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"BsmtQual\": [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"BsmtCond\": [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n    \"BsmtExposure\": [\"None\",\"No\",\"Mn\",\"Av\",\"Gd\"],\n    \"BsmtFinType1\": [\"None\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"],\n    \"BsmtFinType2\": [\"None\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"],\n    \"GarageFinish\": [\"None\",\"Unf\",\"RFn\",\"Fin\"],\n    \"PavedDrive\": [\"N\",\"P\",\"Y\"],\n    \"LotShape\": [\"IR3\",\"IR2\",\"IR1\",\"Reg\"],\n    \"LandSlope\": [\"Sev\",\"Mod\",\"Gtl\"],\n    \"Functional\": [\"Sal\",\"Sev\",\"Maj2\",\"Maj1\",\"Mod\",\"Min2\",\"Min1\",\"Typ\"],\n    \"Utilities\": [\"ELO\",\"NoSeWa\",\"NoSewr\",\"AllPub\"],\n    \"Fence\": [\"None\",\"MnWw\",\"GdWo\",\"MnPrv\",\"GdPrv\"],\n    \"OverallQual\": list(range(1,11)),   # 1..10\n    \"OverallCond\": list(range(1,11))\n}\n\nX = df.drop([\"Id\", \"SalePrice\"], axis=1)\ny = df[\"SalePrice\"]\n\nordinal_cols = [c for c in ORDINAL_ORDER if c in X.columns]\nnominal_cols = [c for c in X.select_dtypes(include=\"object\").columns if c not in ordinal_cols]\n\nreal_numeric = [c for c in X.select_dtypes(include=[\"int64\",\"float64\"]).columns\n                if c not in [\"OverallQual\",\"OverallCond\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:57:25.038011Z","iopub.execute_input":"2025-11-01T08:57:25.038367Z","iopub.status.idle":"2025-11-01T08:57:25.053739Z","shell.execute_reply.started":"2025-11-01T08:57:25.038341Z","shell.execute_reply":"2025-11-01T08:57:25.052653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see how our nominal variables are looking like\n\ndf[nominal_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:57:26.206154Z","iopub.execute_input":"2025-11-01T08:57:26.206477Z","iopub.status.idle":"2025-11-01T08:57:26.231118Z","shell.execute_reply.started":"2025-11-01T08:57:26.206457Z","shell.execute_reply":"2025-11-01T08:57:26.230166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Features and Prediction Target for Our Model","metadata":{}},{"cell_type":"code","source":"# Making the train-validation split.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T08:59:06.056473Z","iopub.execute_input":"2025-11-01T08:59:06.057143Z","iopub.status.idle":"2025-11-01T08:59:06.065152Z","shell.execute_reply.started":"2025-11-01T08:59:06.057113Z","shell.execute_reply":"2025-11-01T08:59:06.064157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:01:01.480685Z","iopub.execute_input":"2025-11-01T09:01:01.481063Z","iopub.status.idle":"2025-11-01T09:01:01.522848Z","shell.execute_reply.started":"2025-11-01T09:01:01.481039Z","shell.execute_reply":"2025-11-01T09:01:01.521788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train data's missing values\nX_train.isnull().sum().sort_values(ascending = False).head(6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:01:19.983887Z","iopub.execute_input":"2025-11-01T09:01:19.984238Z","iopub.status.idle":"2025-11-01T09:01:19.996072Z","shell.execute_reply.started":"2025-11-01T09:01:19.984216Z","shell.execute_reply":"2025-11-01T09:01:19.995017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Our nominal cols in training data is looking like this\n\nX_train[nominal_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:01:22.109721Z","iopub.execute_input":"2025-11-01T09:01:22.110076Z","iopub.status.idle":"2025-11-01T09:01:22.134554Z","shell.execute_reply.started":"2025-11-01T09:01:22.110051Z","shell.execute_reply":"2025-11-01T09:01:22.133367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Identifying Skewed Variables for Log Transformation","metadata":{}},{"cell_type":"markdown","source":"Log transformation is a way to change data that has very large numbers, very small numbers or a skewed shape. It works by taking the logarithm of each number in the data which helps to “compress” the large values and spread out the small ones.","metadata":{}},{"cell_type":"code","source":"# Creating a temporary data to keep the train data same for future use.\nX_train_tmp = X_train[real_numeric].copy()\n\n\n# NA values must be filled because log transformation only accept non-na values . So lets use the median of our columns to fill them. (training data)\nfor col in X_train_tmp.columns:\n    X_train_tmp[col] = X_train_tmp[col].fillna(X_train_tmp[col].median())\n\n# Defining skewed variables\nskewed_features = X_train_tmp.apply(lambda x: x.skew()).sort_values(ascending=False)\nprint(f\"These are the skewed features and their skew value from our data:\\n{skewed_features}\\n\")\nhigh_skewed = skewed_features[skewed_features > 1].index.tolist()\n\nnum_cols_for_log = high_skewed\nnum_cols_no_log = [c for c in real_numeric if c not in num_cols_for_log]\nprint(f\"{num_cols_for_log}\\n\\nThese columns skews are higher than 1\\nWhich means these are highly right skewed, we are going to apply log transformation to them\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:01:27.560443Z","iopub.execute_input":"2025-11-01T09:01:27.560765Z","iopub.status.idle":"2025-11-01T09:01:27.587334Z","shell.execute_reply.started":"2025-11-01T09:01:27.560741Z","shell.execute_reply":"2025-11-01T09:01:27.586262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see how our skewed variables distributed\n\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(12, 6))\n    for i, col in enumerate(skewed_features[skewed_features >1].index[:15]): \n        plt.subplot(3, 5, i+1)\n        sns.histplot(X_train_tmp[col], kde=True)\n        plt.title(f\"{col} (skew={df[col].skew():.2f})\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:03:36.674072Z","iopub.execute_input":"2025-11-01T09:03:36.674395Z","iopub.status.idle":"2025-11-01T09:03:40.310218Z","shell.execute_reply.started":"2025-11-01T09:03:36.674374Z","shell.execute_reply":"2025-11-01T09:03:40.309053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Constructing Our Pipeline for the Model","metadata":{}},{"cell_type":"code","source":"# If a column is numeric and it needed to be applied log transformation\nnumeric_log_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")), # fill the na values with median value\n    (\"log\", FunctionTransformer(func=np.log1p, feature_names_out=\"one-to-one\")),\n])\n\n# If a column is numeric and wont apply log transformation\nnumeric_plain_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    # no log here\n])\n\n# Take the ranked orders from the ordinal order list for every ordinal column and encode it\nordinal_encoder = OrdinalEncoder(\n    categories=[ORDINAL_ORDER[c] for c in ordinal_cols],\n    handle_unknown = \"use_encoded_value\", unknown_value = -1\n)\n\n# It imputes NA values with mode value\n# It encodes each category with an integer value based on its order or ranking\nordinal_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")), # fill the na values with the mode value\n    (\"encode\", ordinal_encoder),\n])\n\n\n# If a column is nominal we have to apply label encoding by using OneHotEncoder\n# Each row is encoded as 1 or 0 to indicate the presence or absence of a category\nnominal_pipeline = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n])\n\n\n# Clustering all of the pipelines in one column transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num_log\",  numeric_log_pipeline, num_cols_for_log),\n        (\"num_plain\", numeric_plain_pipeline, num_cols_no_log),\n        (\"ordinal\", ordinal_pipeline, ordinal_cols),\n        (\"nominal\", nominal_pipeline, nominal_cols),\n    ],\n    remainder=\"passthrough\"\n)\n\n# Defining our model\nmodel = XGBRegressor(random_state = 0)\n\n\n# Constructing the full pipeline including column transformer and the model\n# Including the HouseRules class for applying all of the special applications for our variables\nfull_pipeline = Pipeline(steps=[\n    (\"rules\", HouseRules()),\n    (\"preprocess\", preprocessor),\n    (\"model\", XGBRegressor(random_state=0, n_jobs=-1)),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:04:02.997703Z","iopub.execute_input":"2025-11-01T09:04:02.998353Z","iopub.status.idle":"2025-11-01T09:04:03.008666Z","shell.execute_reply.started":"2025-11-01T09:04:02.998324Z","shell.execute_reply":"2025-11-01T09:04:03.007321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lets fit our training data and target prediction to our pipeline\n\nBy doing this all of these preprocessing steps will be applied:\n*  Log transformation applied to skewed features\n* Numeric NA values filled with the median value\n* Ordinal variables encoded with OrdinalEncoder\n* Nominal variables encoded with OneHotEncoder","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning Using GridSearchCV\n\n\nLet's use GridSearchCV to find best hyperparameters for our XGBoost model.","metadata":{}},{"cell_type":"code","source":"def mae_on_original(y_true_log, y_pred_log):\n    y_true = np.expm1(y_true_log)   \n    y_pred = np.expm1(y_pred_log)\n    return mean_absolute_error(y_true, y_pred)\n\nmae_orig_scorer  = make_scorer(mae_on_original,  greater_is_better=False)\n\nparam_grid = {\n    \"model__n_estimators\": [100, 500, 1000],\n    \"model__learning_rate\": [0.05, 0.1, 0.5],\n    \"model__max_depth\": [4, 6, 8],\n    \"model__subsample\": [0.7, 0.9],\n    \"model__colsample_bytree\": [0.7, 0.9],\n}\n\ngrid = GridSearchCV(\n    estimator = full_pipeline,\n    param_grid = param_grid,\n    cv = 5,\n    scoring = mae_orig_scorer,\n    n_jobs = -1,\n    refit = True,\n    verbose = 0\n)\n\ngrid.fit(X_train, np.log1p(y_train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:04:51.394537Z","iopub.execute_input":"2025-10-31T15:04:51.394713Z","iopub.status.idle":"2025-10-31T15:08:04.383682Z","shell.execute_reply.started":"2025-10-31T15:04:51.394699Z","shell.execute_reply":"2025-10-31T15:08:04.382921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Best hyperparameters and their mean score showed.\n\nresults = pd.DataFrame(grid.cv_results_)\nresults[[\"param_model__colsample_bytree\", \"param_model__learning_rate\", \"param_model__max_depth\", \"param_model__n_estimators\", \"param_model__subsample\", \"mean_test_score\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:08:04.384461Z","iopub.execute_input":"2025-10-31T15:08:04.384701Z","iopub.status.idle":"2025-10-31T15:08:04.396654Z","shell.execute_reply.started":"2025-10-31T15:08:04.384680Z","shell.execute_reply":"2025-10-31T15:08:04.395905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(grid.best_params_)\nprint(\"\\n\",-grid.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:08:04.397582Z","iopub.execute_input":"2025-10-31T15:08:04.397799Z","iopub.status.idle":"2025-10-31T15:08:04.413667Z","shell.execute_reply.started":"2025-10-31T15:08:04.397782Z","shell.execute_reply":"2025-10-31T15:08:04.412954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Since refit=True in our GridSearchCV our best estimators has been saved and applied to our model\n# Thus we are assigning it to a variable\n\nbest_pipeline = grid.best_estimator_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:22:11.693054Z","iopub.execute_input":"2025-11-01T09:22:11.693346Z","iopub.status.idle":"2025-11-01T09:22:11.698127Z","shell.execute_reply.started":"2025-11-01T09:22:11.693324Z","shell.execute_reply":"2025-11-01T09:22:11.696925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see how our train data looking like after we fitted to the pipeline\n# All of the preprocess and encoding methods has been applied\n\nfeature_names = best_pipeline.named_steps[\"preprocess\"].get_feature_names_out()\nX_train_after = pd.DataFrame(\n    best_pipeline.named_steps[\"preprocess\"].transform(X_train),\n    columns=feature_names,\n    index=X_train.index\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:25:35.386240Z","iopub.execute_input":"2025-11-01T09:25:35.386655Z","iopub.status.idle":"2025-11-01T09:25:35.425589Z","shell.execute_reply.started":"2025-11-01T09:25:35.386631Z","shell.execute_reply":"2025-11-01T09:25:35.424618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_after","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:25:37.288021Z","iopub.execute_input":"2025-11-01T09:25:37.289072Z","iopub.status.idle":"2025-11-01T09:25:37.534841Z","shell.execute_reply.started":"2025-11-01T09:25:37.289039Z","shell.execute_reply":"2025-11-01T09:25:37.533496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predicting Test data","metadata":{}},{"cell_type":"code","source":"# Reading the test data\n\ntest_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv.gz\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:17:59.274911Z","iopub.execute_input":"2025-11-01T09:17:59.275305Z","iopub.status.idle":"2025-11-01T09:17:59.304784Z","shell.execute_reply.started":"2025-11-01T09:17:59.275267Z","shell.execute_reply":"2025-11-01T09:17:59.303730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First 5 rows of test data\n\ntest_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:18:11.802180Z","iopub.execute_input":"2025-11-01T09:18:11.803116Z","iopub.status.idle":"2025-11-01T09:18:11.848644Z","shell.execute_reply.started":"2025-11-01T09:18:11.803076Z","shell.execute_reply":"2025-11-01T09:18:11.847272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Our test data has {test_df.shape[0]} rows and {test_df.shape[1]} columns\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:19:00.176817Z","iopub.execute_input":"2025-11-01T09:19:00.177162Z","iopub.status.idle":"2025-11-01T09:19:00.183023Z","shell.execute_reply.started":"2025-11-01T09:19:00.177140Z","shell.execute_reply":"2025-11-01T09:19:00.182026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.isnull().sum().sort_values(ascending=False)[0:20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:19:13.951477Z","iopub.execute_input":"2025-11-01T09:19:13.951800Z","iopub.status.idle":"2025-11-01T09:19:13.964651Z","shell.execute_reply.started":"2025-11-01T09:19:13.951778Z","shell.execute_reply":"2025-11-01T09:19:13.963487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict the test data (predictions are in log scale)\ny_pred_log = grid.predict(test_df)\n\n# Applying the inverse of the log1p transformation to get predictions in the original scale\ny_pred = np.expm1(y_pred_log)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:28:18.486922Z","iopub.execute_input":"2025-11-01T09:28:18.487281Z","iopub.status.idle":"2025-11-01T09:28:18.564043Z","shell.execute_reply.started":"2025-11-01T09:28:18.487257Z","shell.execute_reply":"2025-11-01T09:28:18.563357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"Id\": test_df[\"Id\"],\n    \"SalePrice\": y_pred\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:28:19.131925Z","iopub.execute_input":"2025-11-01T09:28:19.132704Z","iopub.status.idle":"2025-11-01T09:28:19.142922Z","shell.execute_reply.started":"2025-11-01T09:28:19.132671Z","shell.execute_reply":"2025-11-01T09:28:19.141794Z"}},"outputs":[],"execution_count":null}]}